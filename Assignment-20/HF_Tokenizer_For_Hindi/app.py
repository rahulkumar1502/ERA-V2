import gradio as gr
import pickle

with open('merges.pkl','rb') as merges_file:
    merges = pickle.load(merges_file)

with open('vocab.pkl', 'rb') as vocab_file:
    vocab = pickle.load(vocab_file)

def decode(ids):
  tokens = b"".join(vocab[idx] for idx in ids)
  text = tokens.decode("utf-8", errors="replace")
  return text

def encode(text):
  tokens = list(text.encode("utf-8"))
  while len(tokens) >= 2:
    stats = get_stats(tokens)
    pair = min(stats, key=lambda p: merges.get(p, float("inf")))
    if pair not in merges:
      break 
    idx = merges[pair]
    tokens = merge(tokens, pair, idx)
  return tokens

def get_stats(ids):
    counts = {}
    for pair in zip(ids, ids[1:]):
        counts[pair] = counts.get(pair, 0) + 1
    return counts

def merge(ids, pair, idx):
  i = 0
  newids = []
  while i < len(ids):
    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:
      newids.append(idx)
      i += 2
    else:
      newids.append(ids[i])
      i += 1
  
  return newids

def getTokens(inputText):
    tokens = list(inputText.encode("utf-8"))
    while len(tokens) >= 2:
        stats = get_stats(tokens)
        pair = min(stats, key=lambda p: merges.get(p, float("inf")))
        if pair not in merges:
            break 
        idx = merges[pair]
        tokens = merge(tokens, pair, idx)
    strTokens = []
    for token in tokens:
        strTokens.append(decode([token]))
    return tokens, strTokens

color_list = ['rgb(139, 162, 253)', 'rgb(254, 225, 190)', 'rgb(252, 175, 166)', 'rgb(219, 254, 210)', 'rgb(235, 233, 229)', 'rgb(211, 247, 187)', 'rgb(200, 251, 254)', 'rgb(159, 249, 191)', 'rgb(212, 212, 212)', 'rgb(167 243 208)', 'rgb(251 207 232)', 'rgb(243, 165, 240)', 'rgb(239 68 68)', 'rgb(254 205 211)', 'rgb(139, 152, 254)', 'rgb(241 245 249)']
def getColoredText(inputText, tokens, strTokens):
    i = 0
    txtLen = len(inputText)
    numTokens = len(strTokens)
    outputString = """<html><head><style>h2 { font-size: 40px;} sub { vertical-align: sub; font-size: 4px;} span { font-size:16px;}</style></head><body><div><h2>Total characters in text: """
    outputString = outputString + str(txtLen) + """</h2><h2>Token count: """
    outputString = outputString + str(numTokens) + """</h2><p> """
    for strToken in strTokens:
        colorNum = i % len(color_list)
        chosencolor = color_list[colorNum]
        outputString = outputString + "<span style='background-color:" + chosencolor + ";'> " + strToken + "</span>"
        i = i + 1
    outputString = outputString + """</p></body>"""
    return outputString

def tokenizeText(inputText):
    tokens, strTokens = getTokens(inputText)
    return(getColoredText(inputText, tokens, strTokens))
    
title = "Tokenizer for Hindi language from Scratch"
description = "Created Hindi Tokenizer from Scratch"
examples = [
    "‡§ö‡§ø‡§®‡•ç‡§®‡§Ø‡§∞‡§∏‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§≠‡§æ‡§∞‡§§ ‡§ï‡•á ‡§Ü‡§®‡•ç‡§ß‡•ç‡§∞‡§™‡•ç‡§∞‡§¶‡•á‡§∂ ‡§∞‡§æ‡§ú‡•ç‡§Ø ‡§ï‡•á ‡§Ö‡§®‡•ç‡§§‡§∞‡•ç‡§ó‡§§ ‡§ï‡•á ‡§ï‡§°‡§™ ‡§ú‡§ø‡§≤‡•á ‡§ï‡§æ ‡§è‡§ï ‡§ó‡§æ‡§Å‡§µ ‡§π‡•à‡•§", 
    "‡§ï‡§∞‡§æ‡§∏‡•Ä‡§¨‡•Ç‡§Ç‡§ó‡§æ, ‡§ï‡§æ‡§´‡§≤‡•Ä‡§ó‡•à‡§∞ ‡§§‡§π‡§∏‡•Ä‡§≤ ‡§Æ‡•á‡§Ç ‡§≠‡§æ‡§∞‡§§ ‡§ï‡•á ‡§â‡§§‡•ç‡§§‡§∞‡§æ‡§ñ‡§£‡•ç‡§° ‡§∞‡§æ‡§ú‡•ç‡§Ø ‡§ï‡•á ‡§Ö‡§®‡•ç‡§§‡§∞‡•ç‡§ó‡§§ ‡§ï‡•Å‡§Æ‡§æ‡§ä‡§Å ‡§Æ‡§£‡•ç‡§°‡§≤ ‡§ï‡•á ‡§¨‡§æ‡§ó‡•á‡§∂‡•ç‡§µ‡§∞ ‡§ú‡§ø‡§≤‡•á ‡§ï‡§æ ‡§è‡§ï ‡§ó‡§æ‡§Å‡§µ ‡§π‡•à‡•§",
    "‡§á‡§∏ ‡§µ‡§æ‡§ï‡•ç‡§Ø ‡§ï‡§æ ‡§Ö‡§®‡•Å‡§µ‡§æ‡§¶ ‡§ó‡•Ç‡§ó‡§≤100 ‡§ü‡•ç‡§∞‡§æ‡§Ç‡§∏‡§≤‡•á‡§ü ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•à‡•§üòÖ",
    "‡§ï‡•à‡§´‡§º ‡§≠‡•ã‡§™‡§æ‡§≤‡•Ä ‡§®‡•á ‡§ï‡§à ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§´‡§ø‡§≤‡•ç‡§Æ‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§ó‡•Ä‡§§ ‡§≤‡§ø‡§ñ‡•á, ‡§ï‡§ø‡§®‡•ç‡§§‡•Å 1972 ‡§Æ‡•á‡§Ç ‡§¨‡§®‡•Ä ‡§™‡§æ‡§ï‡§º‡•Ä‡§ú‡§º‡§æ ‡§â‡§®‡§ï‡•Ä ‡§Ø‡§æ‡§¶‡§ó‡§æ‡§∞ ‡§´‡§ø‡§≤‡•ç‡§Æ ‡§∞‡§π‡•Ä‡•§",
    "‡§õ‡§æ‡§Ø‡§æ ‡§ó‡§†‡§¨‡§Ç‡§ß‡§® ‡§ï‡•Ä ‡§π‡§æ‡§∞ ‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§µ‡§ø‡§∂‡•á‡§∑ Yamato ‡§¨‡•Ä ‡§¶‡§æ ‡§¨‡•â‡§≤‡•ç‡§∏ ‡§ï‡•Ä ‡§§‡§≤‡§æ‡§∂ ‡§Æ‡•á‡§Ç ‡§π‡•à ‡§∏‡•ç‡§ü‡•ç‡§∞‡§æ‡§á‡§ï ‡§Æ‡§¶‡§™‡§æ‡§®, ‡§ú‡•ã ‡§∏‡§ø‡§§‡§æ‡§∞‡•ã‡§Ç ‡§∂‡•Ç‡§ü‡§ø‡§Ç‡§ó ‡§∏‡•á ‡§â‡§§‡•ç‡§™‡§®‡•ç‡§® ‡§ï‡§π‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à‡•§ ‡§∞‡§π‡§∏‡•ç‡§Ø‡§Æ‡§Ø Haja ‡§ï‡•á ‡§∏‡§æ‡§• ‡§è‡§ï ‡§≤‡§°‡§º‡§æ‡§à ‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§î‡§∞ ‡§â‡§∏‡§ï‡•á ‡§∏‡•ç‡§ü‡•ç‡§∞‡§æ‡§á‡§ï ‡§´‡§ü‡§ï‡§æ, ‡§°‡•ç‡§∞‡§æ‡§á‡§µ ‡§∂‡•Ç‡§ü‡§ø‡§Ç‡§ó ‡§™‡•ç‡§∞‡§æ‡§™‡•ç‡§§ ‡§ï‡§∞‡§®‡•á, Yamato ‡§§‡•ã Gunnos, ‡§è‡§ï ‡§∞‡§Ç‡§ó‡§∞‡•Ç‡§ü ‡§¨‡•Ä DaPlayer ‡§∏‡•á ‡§Æ‡•Å‡§≤‡§æ‡§ï‡§æ‡§§ ‡§ï‡•Ä‡•§ ‡§µ‡§π ‡§Ö‡§™‡§®‡•á ‡§π‡•Ä ‡§π‡§°‡§º‡§§‡§æ‡§≤ ‡§ï‡•ã Yamato ‡§™‡•Å‡§∞‡§æ‡§®‡•á ‡§Æ‡§ø‡§§‡•ç‡§∞‡•ã‡§Ç ‡§î‡§∞ ‡§™‡•ç‡§∞‡§§‡§ø‡§¶‡•ç‡§µ‡§Ç‡§¶‡•ç‡§µ‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•á ‡§∏‡§æ‡§• ‡§µ‡§ø‡§ú‡•á‡§§‡§æ ‡§™‡•ç‡§∞‡§§‡§ø‡§Ø‡•ã‡§ó‡§ø‡§§‡§æ ‡§Æ‡•á‡§Ç ‡§≠‡§æ‡§ó ‡§≤‡•á‡§®‡•á ‡§∂‡•â‡§ü ‡§Æ‡§ø‡§≤ ‡§ó‡§Ø‡§æ‡•§ ‡§≤‡•á‡§ï‡§ø‡§® ‡§ï‡•Å‡§õ ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§ø‡§Ø‡§æ ‡§µ‡•á ‡§™‡§§‡§æ ‡§π‡•à‡•§.. .. ‡§è‡§ï ‡§≠‡§Ø‡§æ‡§®‡§ï ‡§¨‡•Å‡§∞‡§æ‡§à ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ú‡•ã‡§ñ‡§ø‡§Æ ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§¨‡§æ‡§∞ ‡§´‡§ø‡§∞ ‡§¨‡•Ä ‡§¶‡§æ ‡§µ‡§ø‡§∂‡•ç‡§µ ‡§°‡§æ‡§≤ ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§π‡•à‡•§",
    "In 2019 google introduced BERT- Bidirectional Encoder Representations from Transformers (paper), which is designed to pre-train a language model from a vast corpus of rew text. What distinguishes it from existing word-embedding models like Word2vec, ELMo etc. is that it is a truly bidirectional model, meaning it is trained on unlabeled text by jointly conditioning both left and right context simultaneously."
]

demo = gr.Interface(
    tokenizeText, 
    inputs = [
        gr.Textbox(), 
    ],
    outputs = [
        gr.HTML(),
        ],
    title = title,
    description = description,
    examples = examples,
    cache_examples=False
)
demo.launch()