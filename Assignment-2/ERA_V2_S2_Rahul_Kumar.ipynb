{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMVrvsed+JjHuHDd5VfiGQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahulkumar1502/ERA-V2/blob/main/Assignment-2%5CERA_V2_S2_Rahul_Kumar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJptKBxALl-u",
        "outputId": "e46f2828-5f9a-40ec-c4ff-6b60c2a4e76b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n"
          ]
        }
      ],
      "source": [
        "# torch: Core PyTorch library\n",
        "import torch\n",
        "# torch.nn: Building blocks for neural networks\n",
        "import torch.nn as nn\n",
        "# torch.nn.functional: Activation functions and utility functions\n",
        "import torch.nn.functional as F\n",
        "# torch.optim: Optimization algorithms for training\n",
        "import torch.optim as optim\n",
        "# torchvision.datasets: Commonly used datasets\n",
        "# torchvision.transforms: Image transformations\n",
        "from torchvision import datasets, transforms\n",
        "# torchsummary: Visualizing neural network architectures\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checks if a CUDA-enabled GPU is available on the system.\n",
        "# If it is, the code sets the device to use the GPU for computations.\n",
        "# Otherwise, it sets the device to use the CPU.\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00Owi1LBNY8L",
        "outputId": "3ce7c1c8-6ac3-492e-9300-7025e67bca54"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set batch size\n",
        "batch_size = 1024\n",
        "\n",
        "# Training data loader:\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "      # Load MNIST dataset for training\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   # Apply transformations to each data point\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(), # Convert data to PyTorch tensor\n",
        "                        transforms.Normalize((0.1307,), (0.3081,)) # Normalize the data\n",
        "                    ])),\n",
        "    batch_size=batch_size,  # Set batch size\n",
        "    shuffle=True) # Shuffle the data for randomness during training\n",
        "\n",
        "# Testing data loader:\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "     # Load MNIST dataset for testing\n",
        "    datasets.MNIST('../data', train=False,\n",
        "                   # Apply the same transformations as the training data loader\n",
        "                   transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size,  # Set batch size\n",
        "    shuffle=True)  # Shuffle the data for randomness during testing (usually set to False for testing)"
      ],
      "metadata": {
        "id": "EQZaZRGcNLtr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d83878e-a040-4caa-c3db-618ee8a93c5f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 56939847.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 1951629.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 2738282.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 2285606.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some Notes on our naive model\n",
        "\n",
        "We are going to write a network based on what we have learnt so far.\n",
        "\n",
        "The size of the input image is 28x28x1. We are going to add as many layers as required to reach RF = 32 \"atleast\"."
      ],
      "metadata": {
        "id": "r3gEjf-xMb-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FirstDNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(FirstDNN, self).__init__()\n",
        "    # r_in:1, n_in:28, j_in:1, s:1, r_out:3, n_out:28, j_out:1\n",
        "    self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "    # r_in:3 , n_in:32 , j_in: 1, s:1 , r_out: 5, n_out:28 , j_out:1\n",
        "    self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "    # r_in:5 , n_in:64 , j_in: , s:1 , r_out: , n_out:64 , j_out:\n",
        "    self.pool1 = nn.MaxPool2d(2, 2)\n",
        "    # r_in: , n_in:64 , j_in: , s:2 , r_out: , n_out:128 , j_out:\n",
        "    self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "    # r_in: , n_in: , j_in: , s:1 , r_out: , n_out: , j_out:\n",
        "    self.conv4 = nn.Conv2d(128, 256, 3, padding = 1)\n",
        "    # r_in: , n_in: , j_in: , s:1 , r_out: , n_out: , j_out:\n",
        "    self.pool2 = nn.MaxPool2d(2, 2)\n",
        "    # r_in: , n_in: , j_in: , s:2 , r_out: , n_out: , j_out:\n",
        "    self.conv5 = nn.Conv2d(256, 512, 3)\n",
        "    # r_in: , n_in: , j_in: , s:1 , r_out: , n_out: , j_out:\n",
        "    self.conv6 = nn.Conv2d(512, 1024, 3)\n",
        "    # r_in: , n_in: , j_in: , s:1 , r_out: , n_out: , j_out:\n",
        "    self.conv7 = nn.Conv2d(1024, 10, 3)\n",
        "    # Added fully connected layer\n",
        "    self.fc1 = nn.Linear(10, 10)\n",
        "\n",
        "\n",
        "# Correct values\n",
        "# https://user-images.githubusercontent.com/498461/238034116-7db4cec0-7738-42df-8b67-afa971428d39.png\n",
        "  def forward(self, x):\n",
        "    x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x)))))\n",
        "    x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n",
        "    x = F.relu(self.conv6(F.relu(self.conv5(x))))\n",
        "    #x = self.conv7(x)\n",
        "    x = F.relu(self.conv7(x))\n",
        "    #x = F.relu(x) # this is the last step. Think what ReLU does to our results at this stage!\n",
        "    # After above step, all negatives are converted to 0 while remaining tensors are unaffected.\n",
        "    x = x.view(-1, 10) # reshapes the array x to have a single column with 10 rows\n",
        "    return F.log_softmax(x)\n",
        "    # The softmax function is a common activation function used in neural networks.\n",
        "    # It takes a vector of real values as input and normalizes it into a probability distribution consisting of values between 0 and 1 that sum to 1.\n",
        "    # The log_softmax function calculates the logarithm of the softmax values.\n",
        "    # This is useful in certain scenarios such as when training neural networks, as it can improve the numerical stability of the model and make it easier to optimize.\n"
      ],
      "metadata": {
        "id": "Sir2LmSVLr_4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FirstDNN().to(device)"
      ],
      "metadata": {
        "id": "sxICO4TTNt2H"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model, input_size=(1, 28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M__MtFIYNwXa",
        "outputId": "15073fad-7767-4d56-d26a-a7332a6462f8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 28, 28]             320\n",
            "            Conv2d-2           [-1, 64, 28, 28]          18,496\n",
            "         MaxPool2d-3           [-1, 64, 14, 14]               0\n",
            "            Conv2d-4          [-1, 128, 14, 14]          73,856\n",
            "            Conv2d-5          [-1, 256, 14, 14]         295,168\n",
            "         MaxPool2d-6            [-1, 256, 7, 7]               0\n",
            "            Conv2d-7            [-1, 512, 5, 5]       1,180,160\n",
            "            Conv2d-8           [-1, 1024, 3, 3]       4,719,616\n",
            "            Conv2d-9             [-1, 10, 1, 1]          92,170\n",
            "================================================================\n",
            "Total params: 6,379,786\n",
            "Trainable params: 6,379,786\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.51\n",
            "Params size (MB): 24.34\n",
            "Estimated Total Size (MB): 25.85\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-6606815a05df>:37: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train() # Set the model to training mode\n",
        "    pbar = tqdm(train_loader)\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device) # Move the input data and target labels to the specified device (GPU in this case)\n",
        "        optimizer.zero_grad() # Zero out the gradients in the optimizer\n",
        "        output = model(data) # Forward pass: compute predicted outputs by passing input data through the model\n",
        "        loss = F.nll_loss(output, target) # Calculate the negative log likelihood (NLL) loss between the predicted output and target labels\n",
        "        loss.backward()  # Backward pass: compute gradient of the loss with respect to model parameters\n",
        "        optimizer.step() # Update the weights using the optimizer\n",
        "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval() # Set the model to evaluation mode\n",
        "    # Initialize variables to keep track of test loss and correct predictions\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    # Disable gradient computation during testing\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data) # Forward pass: compute predicted outputs by passing input data through the model\n",
        "            # Calculate the negative log likelihood (NLL) loss between the predicted output and target labels\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # Sum up batch loss (reduction='sum') for later calculation of average loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset) # Calculate average test loss\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "id": "g_vlC-bdNzo1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "for epoch in range(1, 2):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0FYVWkGOFBS",
        "outputId": "ea5b04d7-c03a-48e0-e76a-5c38112b4a8a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/59 [00:00<?, ?it/s]<ipython-input-4-6606815a05df>:37: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n",
            "loss=1.7841417789459229 batch_id=58: 100%|██████████| 59/59 [31:56<00:00, 32.49s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.8140, Accuracy: 2822/10000 (28%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6agTEkqzz6TZ"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}